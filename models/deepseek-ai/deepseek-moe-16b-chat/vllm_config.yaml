#Used to configure a vllm backend server for the model. Change as needed.

model: /home/aoifeturner/infinityhub_model_catalog/models/deepseek-ai/deepseek-moe-16b-chat
port: 8000
tokenizer: /home/aoifeturner/infinityhub_model_catalog/models/deepseek-ai/deepseek-moe-16b-chat
dtype: float16
tensor_parallel_size: 1
gpu_memory_utilization: 0.9
max_model_len: 4096
extra_args:
  - --disable-log-requests

